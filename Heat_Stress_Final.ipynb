{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f868f8-1264-438a-a10e-cf230ed1fbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the fully loaded code with the netcdf files and creation of heat stree\\WBGT with the Global World bank datasets.\n",
    "# It will provide the \n",
    "\n",
    "#!pip install netCDF4 h5py h5netcdf xarray rasterio pandas numpy\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from rasterio.transform import from_origin\n",
    "import rasterio\n",
    "\n",
    "# ------------------ USER: set these file paths ------------------\n",
    "# Historical (baseline 2014)\n",
    "tas_hist = r\"C:\\Users\\DELL\\Desktop\\Cognitud\\Heat Stress Project\\timeseries-tasmax-annual-mean_cmip6-x0.25_ensemble-all-historical_timeseries-smooth_median_1950-2014.nc\"\n",
    "hurs_hist = r\"C:\\Users\\DELL\\Desktop\\Cognitud\\Heat Stress Project\\timeseries-hurs-annual-mean_cmip6-x0.25_ensemble-all-historical_timeseries-smooth_median_1950-2014.nc\"\n",
    "\n",
    "# Scenarios (2015-2100)\n",
    "tas_ssp126 = r\"C:\\Users\\DELL\\Desktop\\Cognitud\\Heat Stress Project\\timeseries-tasmax-annual-mean_cmip6-x0.25_ensemble-all-ssp126_timeseries-smooth_median_2015-2100.nc\"\n",
    "tas_ssp245 = r\"C:\\Users\\DELL\\Desktop\\Cognitud\\Heat Stress Project\\timeseries-tasmax-annual-mean_cmip6-x0.25_ensemble-all-ssp245_timeseries-smooth_median_2015-2100.nc\"\n",
    "tas_ssp585 = r\"C:\\Users\\DELL\\Desktop\\Cognitud\\Heat Stress Project\\timeseries-tasmax-annual-mean_cmip6-x0.25_ensemble-all-ssp585_timeseries-smooth_median_2015-2100.nc\"\n",
    "\n",
    "hurs_ssp126 = r\"C:\\Users\\DELL\\Desktop\\Cognitud\\Heat Stress Project\\timeseries-hurs-annual-mean_cmip6-x0.25_ensemble-all-ssp126_timeseries-smooth_median_2015-2100.nc\"\n",
    "hurs_ssp245 = r\"C:\\Users\\DELL\\Desktop\\Cognitud\\Heat Stress Project\\timeseries-hurs-annual-mean_cmip6-x0.25_ensemble-all-ssp245_timeseries-smooth_median_2015-2100.nc\"\n",
    "hurs_ssp585 = r\"C:\\Users\\DELL\\Desktop\\Cognitud\\Heat Stress Project\\timeseries-hurs-annual-mean_cmip6-x0.25_ensemble-all-ssp585_timeseries-smooth_median_2015-2100.nc\"\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "OUT_DIR_BASE = r\"C:\\Users\\DELL\\Desktop\\Cognitud\\Heat Stress Project\\Specific_Maps\"\n",
    "os.makedirs(OUT_DIR_BASE, exist_ok=True)\n",
    "\n",
    "SCENARIOS = {\n",
    "    \"ssp126\": {\"tas\": tas_ssp126, \"hurs\": hurs_ssp126},\n",
    "    \"ssp245\": {\"tas\": tas_ssp245, \"hurs\": hurs_ssp245},\n",
    "    \"ssp585\": {\"tas\": tas_ssp585, \"hurs\": hurs_ssp585},\n",
    "}\n",
    "YEARS = [2030, 2050, 2100]\n",
    "\n",
    "# ---------------- Helpers & thresholds ----------------\n",
    "def open_ds_safe(ncfile):\n",
    "    if not os.path.exists(ncfile):\n",
    "        raise FileNotFoundError(f\"NetCDF not found: {ncfile}\")\n",
    "    for eng in (\"netcdf4\", \"h5netcdf\"):\n",
    "        try:\n",
    "            return xr.open_dataset(ncfile, engine=eng, decode_cf=True)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return xr.open_dataset(ncfile, decode_cf=True)\n",
    "\n",
    "def normalize_latlon(ds):\n",
    "    rename = {}\n",
    "    if \"latitude\" in ds.coords:  rename[\"latitude\"]  = \"lat\"\n",
    "    if \"Latitude\" in ds.coords:  rename[\"Latitude\"]  = \"lat\"\n",
    "    if \"longitude\" in ds.coords: rename[\"longitude\"] = \"lon\"\n",
    "    if \"Longitude\" in ds.coords: rename[\"Longitude\"] = \"lon\"\n",
    "    if rename:\n",
    "        ds = ds.rename(rename)\n",
    "    return ds\n",
    "\n",
    "def select_nearest_year(da, target_year):\n",
    "    time_name = None\n",
    "    for cand in (\"time\",\"year\",\"Year\"):\n",
    "        if cand in da.coords:\n",
    "            time_name = cand\n",
    "            break\n",
    "    if time_name is None:\n",
    "        return da\n",
    "    coord = da[time_name]\n",
    "    if np.issubdtype(coord.dtype, np.datetime64):\n",
    "        years = np.array([int(str(t.astype('datetime64[Y]'))[:4]) for t in coord.values])\n",
    "    else:\n",
    "        years = np.array([int(v) for v in coord.values])\n",
    "    idx = int(np.argmin(np.abs(years - target_year)))\n",
    "    return da.isel({time_name: idx})\n",
    "\n",
    "def write_tif(path, array, lons, lats, nodata, dtype, crs=\"EPSG:4326\"):\n",
    "    lats = np.asarray(lats); lons = np.asarray(lons)\n",
    "    if len(lons) < 2 or len(lats) < 2:\n",
    "        raise ValueError(\"Need at least 2 unique lons and lats to set resolution\")\n",
    "    res_lon = float(lons[1] - lons[0]); res_lat = float(lats[0] - lats[1])\n",
    "    transform = from_origin(float(lons.min()), float(lats.max()), res_lon, res_lat)\n",
    "    profile = {\n",
    "        \"driver\": \"GTiff\",\n",
    "        \"height\": array.shape[0],\n",
    "        \"width\": array.shape[1],\n",
    "        \"count\": 1,\n",
    "        \"crs\": crs,\n",
    "        \"transform\": transform,\n",
    "        \"dtype\": dtype,\n",
    "        \"nodata\": nodata\n",
    "    }\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with rasterio.open(path, \"w\", **profile) as dst:\n",
    "        dst.write(array, 1)\n",
    "\n",
    "# Category A (Comfortable .. Danger) - used for scenario zones\n",
    "def wbgt_category_A(wbgt):\n",
    "    if wbgt < 22:\n",
    "        return \"Very low\"\n",
    "    elif wbgt <= 24:\n",
    "        return \"Low\"\n",
    "    elif wbgt <= 30:\n",
    "        return \"Moderate\"\n",
    "    elif wbgt <= 33:\n",
    "        return \"High\"\n",
    "    else:\n",
    "        return \"Very high\"\n",
    "\n",
    "def wbgt_zones_A(wb):\n",
    "    codes = np.zeros_like(wb, dtype=np.int16)\n",
    "    codes[wb < 22] = 1\n",
    "    codes[(wb >= 22) & (wb <= 24)] = 2\n",
    "    codes[(wb > 24) & (wb <= 30)] = 3\n",
    "    codes[(wb > 30) & (wb <= 33)] = 4\n",
    "    codes[wb > 33] = 5\n",
    "    return codes\n",
    "\n",
    "# Baseline intensity (Very low .. Very high) - used for baseline zones\n",
    "def wbgt_intensity_2014_label(wbgt):\n",
    "    if wbgt < 22:\n",
    "        return \"Very low\"\n",
    "    elif wbgt <= 24:\n",
    "        return \"Low\"\n",
    "    elif wbgt <= 30:\n",
    "        return \"Moderate\"\n",
    "    elif wbgt <= 33:\n",
    "        return \"High\"\n",
    "    else:\n",
    "        return \"Very high\"\n",
    "\n",
    "def wbgt_intensity_2014_code(wb):\n",
    "    codes = np.zeros_like(wb, dtype=np.int16)\n",
    "    codes[wb < 22] = 1\n",
    "    codes[(wb >= 22) & (wb <= 24)] = 2\n",
    "    codes[(wb > 24) & (wb <= 30)] = 3\n",
    "    codes[(wb > 30) & (wb <= 33)] = 4\n",
    "    codes[wb > 33] = 5\n",
    "    return codes\n",
    "\n",
    "# ---------------- Process baseline 2014 (global) ----------------\n",
    "print(\"Processing baseline (2014)...\")\n",
    "ds_tas_hist = open_ds_safe(tas_hist); ds_tas_hist = normalize_latlon(ds_tas_hist)\n",
    "ds_hurs_hist = open_ds_safe(hurs_hist); ds_hurs_hist = normalize_latlon(ds_hurs_hist)\n",
    "\n",
    "tas_var = list(ds_tas_hist.data_vars.keys())[0]\n",
    "hurs_var = list(ds_hurs_hist.data_vars.keys())[0]\n",
    "da_tas_hist = ds_tas_hist[tas_var]\n",
    "da_hurs_hist = ds_hurs_hist[hurs_var]\n",
    "\n",
    "tas_2014 = select_nearest_year(da_tas_hist, 2014)\n",
    "hurs_2014 = select_nearest_year(da_hurs_hist, 2014)\n",
    "\n",
    "tas_df = tas_2014.to_dataframe().reset_index()\n",
    "hurs_df = hurs_2014.to_dataframe().reset_index()\n",
    "\n",
    "tas_col = tas_2014.name if tas_2014.name else \"tas\"\n",
    "hurs_col = hurs_2014.name if hurs_2014.name else \"hurs\"\n",
    "tas_df = tas_df.rename(columns={tas_2014.name: tas_col})\n",
    "hurs_df = hurs_df.rename(columns={hurs_2014.name: hurs_col})\n",
    "\n",
    "global_df = pd.merge(tas_df, hurs_df, on=[\"lat\",\"lon\"], how=\"inner\")\n",
    "\n",
    "if global_df[hurs_col].max() <= 1.0 + 1e-6:\n",
    "    global_df[hurs_col] = global_df[hurs_col] * 100.0\n",
    "if global_df[tas_col].mean() > 150:\n",
    "    global_df[tas_col] = global_df[tas_col] - 273.15\n",
    "\n",
    "# WBGT\n",
    "T = global_df[tas_col].astype(float)\n",
    "RH = global_df[hurs_col].astype(float)\n",
    "e = (RH / 100.0) * 6.105 * np.exp((17.27 * T) / (237.7 + T))\n",
    "global_df[\"WBGT_Liljegren\"] = 0.567 * T + 0.393 * e + 3.94\n",
    "\n",
    "# Baseline intensity zones (codes 1..5) + labels\n",
    "global_df[\"WBGT_Intensity_Label\"] = global_df[\"WBGT_Liljegren\"].apply(wbgt_intensity_2014_label)\n",
    "global_df[\"WBGT_Zones_1to5\"] = wbgt_intensity_2014_code(global_df[\"WBGT_Liljegren\"].values)\n",
    "\n",
    "# Save baseline CSV\n",
    "base_dir = os.path.join(OUT_DIR_BASE, \"baseline_2014\")\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "out_csv_base = os.path.join(base_dir, \"baseline_wbgt_2014.csv\")\n",
    "global_df.to_csv(out_csv_base, index=False)\n",
    "\n",
    "# Rasterize baseline WBGT + zones (int codes)\n",
    "lats = np.sort(global_df[\"lat\"].unique())[::-1]\n",
    "lons = np.sort(global_df[\"lon\"].unique())\n",
    "\n",
    "wbgt_grid = global_df.pivot(index=\"lat\", columns=\"lon\", values=\"WBGT_Liljegren\").reindex(index=lats, columns=lons).values\n",
    "zones_grid = global_df.pivot(index=\"lat\", columns=\"lon\", values=\"WBGT_Zones_1to5\").reindex(index=lats, columns=lons).values\n",
    "\n",
    "wbgt_grid = np.asarray(wbgt_grid, dtype=np.float32)\n",
    "zones_grid = np.asarray(zones_grid, dtype=np.float32)\n",
    "\n",
    "out_wbgt_tif = os.path.join(base_dir, \"wbgt_baseline_2014.tif\")\n",
    "out_zones_tif = os.path.join(base_dir, \"wbgt_zones_baseline_2014.tif\")\n",
    "write_tif(out_wbgt_tif, np.where(np.isnan(wbgt_grid), np.nan, wbgt_grid), lons, lats, nodata=np.nan, dtype=\"float32\")\n",
    "write_tif(out_zones_tif, np.where(np.isnan(zones_grid), 0, zones_grid).astype(np.int16), lons, lats, nodata=0, dtype=\"int16\")\n",
    "\n",
    "print(\"Saved baseline maps:\", out_wbgt_tif, out_zones_tif)\n",
    "\n",
    "# ---------------- Process each scenario-year (global) ----------------\n",
    "for scen, files in SCENARIOS.items():\n",
    "    print(f\"\\nProcessing scenario: {scen}\")\n",
    "    try:\n",
    "        ds_tas = open_ds_safe(files[\"tas\"]); ds_tas = normalize_latlon(ds_tas)\n",
    "        ds_hurs = open_ds_safe(files[\"hurs\"]); ds_hurs = normalize_latlon(ds_hurs)\n",
    "    except Exception as e:\n",
    "        print(f\" Failed to open files for {scen}: {e}\")\n",
    "        continue\n",
    "\n",
    "    tas_var = list(ds_tas.data_vars.keys())[0]\n",
    "    hurs_var = list(ds_hurs.data_vars.keys())[0]\n",
    "    da_tas = ds_tas[tas_var]\n",
    "    da_hurs = ds_hurs[hurs_var]\n",
    "\n",
    "    for year in YEARS:\n",
    "        print(f\" Year: {year}\")\n",
    "        try:\n",
    "            tas_yr = select_nearest_year(da_tas, year)\n",
    "            hurs_yr = select_nearest_year(da_hurs, year)\n",
    "        except Exception as e:\n",
    "            print(f\"  Year selection error: {e}\")\n",
    "            continue\n",
    "\n",
    "        tas_df = tas_yr.to_dataframe().reset_index()\n",
    "        hurs_df = hurs_yr.to_dataframe().reset_index()\n",
    "        tas_col = tas_yr.name if tas_yr.name else \"tas\"\n",
    "        hurs_col = hurs_yr.name if hurs_yr.name else \"hurs\"\n",
    "        tas_df = tas_df.rename(columns={tas_yr.name: tas_col})\n",
    "        hurs_df = hurs_df.rename(columns={hurs_yr.name: hurs_col})\n",
    "\n",
    "        global_df = pd.merge(tas_df, hurs_df, on=[\"lat\",\"lon\"], how=\"inner\")\n",
    "\n",
    "        if global_df[hurs_col].max() <= 1.0 + 1e-6:\n",
    "            global_df[hurs_col] = global_df[hurs_col] * 100.0\n",
    "        if global_df[tas_col].mean() > 150:\n",
    "            global_df[tas_col] = global_df[tas_col] - 273.15\n",
    "\n",
    "        # WBGT\n",
    "        T = global_df[tas_col].astype(float)\n",
    "        RH = global_df[hurs_col].astype(float)\n",
    "        e = (RH / 100.0) * 6.105 * np.exp((17.27 * T) / (237.7 + T))\n",
    "        global_df[\"WBGT_Liljegren\"] = 0.567 * T + 0.393 * e + 3.94\n",
    "\n",
    "        # Category A zones for scenarios (Comfortable -> Danger)\n",
    "        global_df[\"Heat_Stress_Category\"] = global_df[\"WBGT_Liljegren\"].apply(wbgt_category_A)\n",
    "        global_df[\"WBGT_Zones_1to5\"] = wbgt_zones_A(global_df[\"WBGT_Liljegren\"].values)\n",
    "\n",
    "        # Save CSV & TIFFs in scenario/year folder\n",
    "        outdir = os.path.join(OUT_DIR_BASE, scen, str(year))\n",
    "        os.makedirs(outdir, exist_ok=True)\n",
    "        out_csv = os.path.join(outdir, f\"{scen}_{year}_wbgt.csv\")\n",
    "        global_df.to_csv(out_csv, index=False)\n",
    "\n",
    "        lats = np.sort(global_df[\"lat\"].unique())[::-1]\n",
    "        lons = np.sort(global_df[\"lon\"].unique())\n",
    "        if len(lats) < 2 or len(lons) < 2:\n",
    "            print(\"  Not enough grid points to rasterize; CSV saved only.\")\n",
    "            continue\n",
    "\n",
    "        wbgt_grid = global_df.pivot(index=\"lat\", columns=\"lon\", values=\"WBGT_Liljegren\").reindex(index=lats, columns=lons).values\n",
    "        zones_grid = global_df.pivot(index=\"lat\", columns=\"lon\", values=\"WBGT_Zones_1to5\").reindex(index=lats, columns=lons).values\n",
    "\n",
    "        wbgt_grid = np.asarray(wbgt_grid, dtype=np.float32)\n",
    "        zones_grid = np.asarray(zones_grid, dtype=np.float32)\n",
    "\n",
    "        out_wbgt = os.path.join(outdir, f\"{scen}_{year}_wbgt.tif\")\n",
    "        out_zones = os.path.join(outdir, f\"{scen}_{year}_zones.tif\")\n",
    "        write_tif(out_wbgt, np.where(np.isnan(wbgt_grid), np.nan, wbgt_grid), lons, lats, nodata=np.nan, dtype=\"float32\")\n",
    "        write_tif(out_zones, np.where(np.isnan(zones_grid), 0, zones_grid).astype(np.int16), lons, lats, nodata=0, dtype=\"int16\")\n",
    "\n",
    "        print(\"  Saved:\", out_wbgt, out_zones, \"and CSV:\", out_csv)\n",
    "\n",
    "print(\"\\nDone. All specific TIFFs/CSVs are under:\", OUT_DIR_BASE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd091ec-3fe5-4f5e-b2d8-93f15f7142bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "batch_clip_recursive.py\n",
    "\n",
    "Recursively clip TIFFs under your Specific_Maps folder by the provided shapefile.\n",
    "Only writes clipped TIFFs (no PNGs).\n",
    "\"\"\"\n",
    "## I will clip it only the land area of the stress using the Shape file.\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "from rasterio.enums import ColorInterp\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import mapping, box, shape\n",
    "\n",
    "# ===================== USER PATHS (EDIT if needed) =====================\n",
    "MAPS_ROOT = r\"C:\\Users\\DELL\\Desktop\\Cognitud\\Heat Stress Project\\Specific_Maps\"\n",
    "SHAPEFILE = r\"C:\\Users\\DELL\\Desktop\\Cognitud\\world_shp\\geoBoundariesCGAZ_ADM0\\geoBoundariesCGAZ_ADM0.shp\"\n",
    "OUT_ROOT  = r\"C:\\Users\\DELL\\Desktop\\Cognitud\\Heat Stress Project\\Specific_Maps\\Clipped_Output\"\n",
    "# =====================================================================\n",
    "\n",
    "os.makedirs(OUT_ROOT, exist_ok=True)\n",
    "\n",
    "# Parameters\n",
    "CROP_TO_SHAPE = True   # True -> crop to polygon bbox; False -> keep original extent & mask\n",
    "USE_UNION_GEOM = True  # dissolve all polygons into one\n",
    "\n",
    "# Load shapefile and dissolve to one polygon (union)\n",
    "gdf = gpd.read_file(SHAPEFILE)\n",
    "if gdf.empty:\n",
    "    raise RuntimeError(\"Shapefile empty or missing. Check path and .shx/.dbf files.\")\n",
    "union_geom = gdf.unary_union if USE_UNION_GEOM else gdf.geometry\n",
    "\n",
    "# find tifs recursively\n",
    "tif_paths = sorted(Path(MAPS_ROOT).rglob(\"*.tif\"))\n",
    "if not tif_paths:\n",
    "    raise SystemExit(f\"No .tif files found under: {MAPS_ROOT}\")\n",
    "\n",
    "print(f\"Found {len(tif_paths)} TIFF(s). Processing...\")\n",
    "\n",
    "for tif in tif_paths:\n",
    "    rel = tif.relative_to(MAPS_ROOT)\n",
    "    out_tif = Path(OUT_ROOT) / rel.parent / f\"clipped_{tif.name}\"\n",
    "    out_tif.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(\"\\n----\")\n",
    "    print(\"Input:\", tif)\n",
    "    try:\n",
    "        with rasterio.open(tif) as src:\n",
    "            src_nodata = src.nodatavals[0] if src.nodatavals else None\n",
    "\n",
    "            # Reproject vector to raster CRS if needed\n",
    "            if gdf.crs != src.crs:\n",
    "                geom_reproj = gpd.GeoSeries([union_geom], crs=gdf.crs).to_crs(src.crs).iloc[0]\n",
    "                geom_for_mask = [mapping(geom_reproj)]\n",
    "            else:\n",
    "                geom_for_mask = [mapping(union_geom)]\n",
    "\n",
    "            # Quick intersection check\n",
    "            raster_box = box(*src.bounds)\n",
    "            shp_shape = shape(geom_for_mask[0])\n",
    "            if not raster_box.intersects(shp_shape):\n",
    "                warnings.warn(\" -> No intersection. Skipping this tile.\")\n",
    "                continue\n",
    "\n",
    "            # Perform mask\n",
    "            clipped, out_transform = mask(\n",
    "                src, geom_for_mask,\n",
    "                crop=CROP_TO_SHAPE, filled=True, nodata=src_nodata\n",
    "            )\n",
    "\n",
    "            # Count valid pixels\n",
    "            valid_pixels = 0\n",
    "            for b in range(clipped.shape[0]):\n",
    "                band = clipped[b]\n",
    "                if src_nodata is not None:\n",
    "                    valid = (band != src_nodata).sum()\n",
    "                else:\n",
    "                    valid = (~(band != band)).sum()  # count non-NaN\n",
    "                valid_pixels = max(valid_pixels, valid)\n",
    "\n",
    "            if valid_pixels == 0:\n",
    "                warnings.warn(\"Clipped result contains 0 valid pixels. Skipping write.\")\n",
    "                continue\n",
    "\n",
    "            # Write output and attempt to preserve colormap for 1-band paletted rasters\n",
    "            profile = src.profile.copy()\n",
    "            profile.update({\n",
    "                \"height\": clipped.shape[1],\n",
    "                \"width\": clipped.shape[2],\n",
    "                \"transform\": out_transform,\n",
    "                \"count\": clipped.shape[0],\n",
    "            })\n",
    "            if src_nodata is not None:\n",
    "                profile[\"nodata\"] = src_nodata\n",
    "\n",
    "            with rasterio.open(out_tif, \"w\", **profile) as dst:\n",
    "                dst.write(clipped)\n",
    "                try:\n",
    "                    cm = src.colormap(1)\n",
    "                    if cm:\n",
    "                        dst.write_colormap(1, cm)\n",
    "                        dst.colorinterp = (ColorInterp.palette,)\n",
    "                        print(\" Preserved color map (palette).\")\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "            print(\" ✅ Wrote:\", out_tif)\n",
    "\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"Failed processing {tif}: {e}\")\n",
    "\n",
    "print(\"\\nAll done. Outputs in:\", OUT_ROOT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af7653f-688b-4536-8126-ee194f573d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Smoothening the Pixels for better look\n",
    "import os\n",
    "import rasterio\n",
    "from rasterio.features import shapes, rasterize\n",
    "from shapely.geometry import shape\n",
    "import geopandas as gpd\n",
    "from rasterio.transform import Affine\n",
    "import numpy as np\n",
    "\n",
    "# ========== CONFIG ==========\n",
    "IN_DIR  = r\"C:\\Users\\DELL\\Desktop\\Cognitud\\Heat Stress Project\\OK Maps\"\n",
    "OUT_DIR = r\"C:\\Users\\DELL\\Desktop\\Cognitud\\Heat Stress Project\\OK Maps_smooth\"\n",
    "# If you prefer SMOOTH in pixels, set USE_PIXELS=True and specify SMOOTH_PIXELS.\n",
    "USE_PIXELS = True\n",
    "SMOOTH_PIXELS = 2         # how many pixels to smooth (try 1,2,3,5)\n",
    "# If USE_PIXELS=False then SMOOTH is interpreted in CRS units (degrees or meters)\n",
    "SMOOTH = 1.5\n",
    "# =================================\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "def pixel_size(transform: Affine):\n",
    "    \"\"\"Return approximate pixel size (max of xres and abs(yres)) in CRS units\"\"\"\n",
    "    xres = abs(transform.a)\n",
    "    yres = abs(transform.e)\n",
    "    return max(xres, yres)\n",
    "\n",
    "def smooth_edges(data, transform, nodata=None, smooth_pixels=None, smooth_crs=None):\n",
    "    # Mask = all valid pixels (not nodata)\n",
    "    if nodata is not None:\n",
    "        valid_mask = (data != nodata)\n",
    "    else:\n",
    "        # treat NaN as nodata for float rasters, otherwise consider >0 as valid\n",
    "        if np.issubdtype(data.dtype, np.floating):\n",
    "            valid_mask = ~np.isnan(data)\n",
    "        else:\n",
    "            valid_mask = (data != 0)\n",
    "\n",
    "    # Vectorize zones\n",
    "    results = (\n",
    "        {\"properties\": {\"val\": v}, \"geometry\": s}\n",
    "        for s, v in shapes(data, mask=valid_mask, transform=transform)\n",
    "    )\n",
    "    gdf = gpd.GeoDataFrame.from_features(results)\n",
    "\n",
    "    if gdf.empty:\n",
    "        # nothing to do\n",
    "        return data.copy()\n",
    "\n",
    "    # World outline = union of all polygons (cleaned)\n",
    "    world_outline = gdf.unary_union.buffer(0)\n",
    "\n",
    "    # determine buffer distance to use\n",
    "    if smooth_pixels is not None:\n",
    "        # convert pixel count -> CRS units\n",
    "        px = pixel_size(transform)\n",
    "        buf = smooth_pixels * px\n",
    "    else:\n",
    "        buf = smooth_crs if smooth_crs is not None else SMOOTH\n",
    "\n",
    "    # Smooth polygons using buffer in/out\n",
    "    # use buffer(,) on GeoDataFrame (applies per row) — safer if many classes\n",
    "    try:\n",
    "        gdf[\"geometry\"] = gdf.geometry.buffer(buf).buffer(-buf)\n",
    "    except Exception:\n",
    "        # fallback: operate on unary_union per geometry (slower)\n",
    "        gdf[\"geometry\"] = [geom.buffer(buf).buffer(-buf) if geom is not None else None for geom in gdf.geometry]\n",
    "\n",
    "    # Clip smoothed polygons back to world outline\n",
    "    gdf[\"geometry\"] = gdf[\"geometry\"].intersection(world_outline)\n",
    "\n",
    "    # Rasterize back. Use nodata as fill if provided, else 0.\n",
    "    fillval = nodata if nodata is not None else 0\n",
    "    # Build list of (geom, val) skipping empty geometries\n",
    "    items = [(geom, val) for geom, val in zip(gdf.geometry, gdf[\"val\"]) if geom is not None and not geom.is_empty]\n",
    "\n",
    "    if not items:\n",
    "        return data.copy()\n",
    "\n",
    "    out_data = rasterize(\n",
    "        items,\n",
    "        out_shape=data.shape,\n",
    "        transform=transform,\n",
    "        fill=fillval,\n",
    "        dtype=data.dtype,\n",
    "    )\n",
    "    return out_data\n",
    "\n",
    "# ========== LOOP OVER ALL TIFFS ==========\n",
    "for fname in sorted(os.listdir(IN_DIR)):\n",
    "    if not fname.lower().endswith(\".tif\"):\n",
    "        continue\n",
    "\n",
    "    in_path  = os.path.join(IN_DIR, fname)\n",
    "    out_path = os.path.join(OUT_DIR, fname.replace(\".tif\", \"_smooth.tif\"))\n",
    "\n",
    "    with rasterio.open(in_path) as src:\n",
    "        data = src.read(1)\n",
    "        profile = src.profile.copy()\n",
    "        transform = src.transform\n",
    "        nodata = profile.get(\"nodata\", None)\n",
    "\n",
    "        if USE_PIXELS:\n",
    "            smoothed = smooth_edges(data, transform, nodata=nodata, smooth_pixels=SMOOTH_PIXELS)\n",
    "        else:\n",
    "            smoothed = smooth_edges(data, transform, nodata=nodata, smooth_crs=SMOOTH)\n",
    "\n",
    "        # write out (preserve dtype and nodata)\n",
    "        profile.update(dtype=smoothed.dtype, count=1)\n",
    "        if nodata is not None:\n",
    "            profile[\"nodata\"] = nodata\n",
    "\n",
    "    with rasterio.open(out_path, \"w\", **profile) as dst:\n",
    "        dst.write(smoothed, 1)\n",
    "\n",
    "    print(f\"✅ Smoothed saved: {out_path}\")\n",
    "\n",
    "print(\"🎯 All TIFFs processed — smooth edges, world boundary preserved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
